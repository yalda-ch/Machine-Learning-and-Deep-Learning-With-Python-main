{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Comparative Analysis of Binary and Frequency Bag-of-Words Representations for Medical Transcript Classification Using Machine Learning Models"
      ],
      "metadata": {
        "id": "YHB06rlaDMld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uploading and Using Train, Test, and Validation CSV Files in Google Colab :**\n",
        "\n",
        "This project consists of three CSV files: train, test, and valid. The code uses Google Colab's files.upload() function to upload these files from the local computer into the Colab environment, where they can be accessed and used for tasks such as loading data for training, testing, and validating a machine learning model."
      ],
      "metadata": {
        "id": "FXrsSLFqv7Tu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "DiQ9WcySBqAj",
        "outputId": "03287d68-6e56-4387-c54e-dbf6e16ca6c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cf170ee9-6ae1-43c4-aa6d-e7fe0605ebf1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cf170ee9-6ae1-43c4-aa6d-e7fe0605ebf1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train (1).csv\n",
            "Saving test.csv to test (1).csv\n",
            "Saving valid.csv to valid (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries :**\n",
        "\n",
        "This code imports essential libraries for data analysis, preprocessing, and machine learning.\n",
        "\n",
        "* pandas and numpy handle data manipulation and numerical operations.\n",
        "\n",
        "* re and string help with text processing and cleaning.\n",
        "\n",
        "* Counter is used for counting elements (e.g., word frequencies).\n",
        "\n",
        "* LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, and XGBClassifier are machine learning models for classification tasks.\n",
        "\n",
        "* f1_score evaluates model performance.\n",
        "\n",
        "* tqdm provides a progress bar for loops to track execution progress."
      ],
      "metadata": {
        "id": "QY0yLWAXwnKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "87ZQ6A8ug0nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Prepare Data :**\n",
        "\n",
        "This part of the code loads the training, validation, and test datasets from three separate CSV files using pandas.read_csv(). After loading the data into DataFrames, it extracts the text column from each file, converts the values to strings, and stores them as lists. It also extracts the label column and stores it as lists as well. As a result, we have train_texts and train_labels for model training, valid_texts and valid_labels for model validation, and test_texts and test_labels for final model evaluation. This prepares the data in a convenient format for machine learning tasks."
      ],
      "metadata": {
        "id": "BEI347j1xqh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "valid_df = pd.read_csv(\"valid.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "valid_texts = valid_df['text'].astype(str).tolist()\n",
        "valid_labels = valid_df['label'].tolist()\n",
        "test_texts  = test_df['text'].astype(str).tolist()\n",
        "test_labels = test_df['label'].tolist()"
      ],
      "metadata": {
        "id": "DjKNBzA7E3Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing (Lowercasing and Punctuation Removal) :**\n",
        "\n",
        "This part of the code focuses on preparing the raw text data so that it is clean and consistent before being used in a machine learning model. It defines a function called preprocess() that takes a text string as input and applies three key transformations: (1) it converts all characters to lowercase, ensuring that words like “Good” and “good” are treated the same; (2) it removes all punctuation marks by replacing them with spaces, which helps avoid treating punctuation as separate tokens; and (3) it removes any extra spaces so that the final text is neatly formatted. After defining this function, it is applied to every text sample in the training, validation, and test datasets. This step is crucial because it reduces noise, standardizes the input data, and helps the machine learning models focus on meaningful patterns rather than irrelevant differences caused by case sensitivity or punctuation."
      ],
      "metadata": {
        "id": "PxPkhqkqydpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocessing (lowercase + remove punctuation)\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "train_texts = [preprocess(t) for t in train_texts]\n",
        "valid_texts = [preprocess(t) for t in valid_texts]\n",
        "test_texts  = [preprocess(t) for t in test_texts]"
      ],
      "metadata": {
        "id": "aKd2bUBtIrRZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a Top-10,000 Word Vocabulary from Training Texts :**\n",
        "\n",
        "This code snippet is responsible for constructing a vocabulary of the most frequent words from the training dataset, which is a critical step in text vectorization for the medical transcript classification task. The goal here is to identify the top 10,000 words that appear most often in the training texts and assign each word a unique ID, along with recording its frequency.\n",
        "\n",
        "The process begins by initializing a Counter object from Python’s collections module. This object, word_counts, is used to tally the occurrences of every word across all training texts. The code loops over each training text in train_texts and splits it into words using whitespace as the delimiter. For each text, word_counts.update() updates the counts for each word, ensuring that the frequency of each word is accurately accumulated across the entire training set.\n",
        "\n",
        "After counting all words, the variable TOP_K is set to 10,000 to limit the vocabulary to the most frequent 10,000 words. The most_common(TOP_K) function is used to retrieve these words in descending order of frequency. These words are stored in the list vocab, which now represents the fixed-size vocabulary.\n",
        "\n",
        "Next, the code creates a mapping from words to unique integer IDs using a dictionary comprehension: word2id = {word: i for i, word in enumerate(vocab)}. Here, each word in the top-10,000 vocabulary is assigned an ID starting from 0. This mapping will later be used to convert texts into numerical vectors, either for binary bag-of-words or frequency bag-of-words representations.\n",
        "\n",
        "To persist the vocabulary, the code writes the word, its ID, and its frequency into a file named vocab.txt. Each line in this file corresponds to one word and follows the format: word ID frequency. This ensures that the vocabulary can be reused consistently for vectorization of validation and test sets without leaking information from them.\n",
        "\n",
        "Finally, the code prints the first ten words from the vocabulary along with their IDs and frequencies. The output confirms that common English words like “the,” “and,” and “was” dominate the top of the list, which aligns with typical word distributions in English medical texts. For example, the word “the” has an ID of 0 and occurs 118,887 times in the training dataset."
      ],
      "metadata": {
        "id": "mXdpsa4mzNRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Build Vocabulary (Top 10,000 words from TRAIN)\n",
        "word_counts = Counter()\n",
        "for t in train_texts:\n",
        "    word_counts.update(t.split())\n",
        "TOP_K = 10000\n",
        "vocab = [word for word, _ in word_counts.most_common(TOP_K)]\n",
        "word2id = {word: i for i, word in enumerate(vocab)}  # ids start from 0\n",
        "with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, word in enumerate(vocab):\n",
        "        f.write(f\"{word} {i} {word_counts[word]}\\n\")\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{word} {word2id[word]} {word_counts[word]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCmgfaNMlrOq",
        "outputId": "07312a1c-6d89-4cc7-c937-0e837944a194"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 0 118887\n",
            "and 1 66917\n",
            "was 2 56124\n",
            "of 3 48447\n",
            "to 4 41003\n",
            "a 5 34316\n",
            "with 6 28462\n",
            "in 7 26243\n",
            "is 8 21651\n",
            "patient 9 19289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting Texts to Word ID Representations :**\n",
        "\n",
        "This code converts raw texts into numerical word ID sequences using the previously built vocabulary, making them ready for machine learning models. The convert_to_ids function takes texts, labels, and a filename, then iterates over each text-label pair. Words present in the vocabulary are replaced with their IDs, out-of-vocabulary words are ignored, and the class label is appended at the end. Each processed line is written to a file (train_ids.txt, valid_ids.txt, test_ids.txt) and stored in memory.\n",
        "\n",
        "The function is applied to the training, validation, and test sets, ensuring consistent preprocessing. Printing the first five lines of the training set confirms that texts are correctly converted into sequences of IDs with labels, ready for classification."
      ],
      "metadata": {
        "id": "hCVZL7hK1TCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Save Train/Valid/Test with Word IDs\n",
        "def convert_to_ids(texts, labels, filename):\n",
        "    lines = []\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for text, label in zip(texts, labels):\n",
        "            ids = [str(word2id[w]) for w in text.split() if w in word2id]\n",
        "            line = \" \".join(ids) + f\" {label}\\n\"\n",
        "            f.write(line)\n",
        "            lines.append(line.strip())\n",
        "    return lines\n",
        "train_ids = convert_to_ids(train_texts, train_labels, \"train_ids.txt\")\n",
        "valid_ids = convert_to_ids(valid_texts, valid_labels, \"valid_ids.txt\")\n",
        "test_ids  = convert_to_ids(test_texts, test_labels, \"test_ids.txt\")\n",
        "for line in train_ids[:5]:\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "q2fiU7KdJtSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf15d2b-33bb-4f3b-9786-c646323ac55b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 248 542 27 157 424 232 2588 2253 3912 5154 26 157 21 364 1009 55 33 778 450 36 391 9548 1777 46 33 19 897 40 33 1034 3 0 1829 1 1089 4991 83 33 21 897 1 21 364 778 450 1945 27 29 8 27 4 26 424 1278 1079 163 55 10 424 232 26 157 1829 1278 6 315 157 1060 7 19 105 1381 299 1413 691 1860 1290 27 33 21 897 26 391 9548 1777 36 157 1829 1278 55 315 157 1060 7 19 105 1381 1094 26 248 542 1945 1829 1278 105 1381 232 364 105 897 1829 1278 2\n",
            "137 205 1513 1 1214 2924 123 205 2589 1 1214 2616 34 7931 116 2778 506 3 34 0 2505 3913 2 1095 68 0 892 1 566 618 87 0 867 679 1 1769 4 0 796 399 3 0 1769 0 4992 2 33 1 0 104 1214 2749 2 247 0 1214 2639 2 33 6 0 1752 1045 1913 17 1898 117 44 0 7330 17 16 244 29 14 472 1643 5261 1 5 3284 3 17 1898 117 816 16 29 2 5 148 2240 463 6 0 5478 57 17 2489 117 44 0 7330 0 591 188 0 463 2 33 0 1012 1046 2 33 6 33 591 316 0 3914 2 716 3674 3 0 774 68 0 1769 47 2 33 87 0 796 399 260 3345 3 0 774 520 1270 14 129 3 0 6859 4512 0 2550 3 5 148 2240 463 51 2 1452 4 1678 2081 3 0 99 343 38 490 110 830 2 39 726 0 2616 1 1308 4 0 861 1156 212 0 830 2 1770 1308 0 1046 400 4 32 2679 214 490 110 1156 2949 24 0 2616 2 7 1387 35 5 131 101 10 5730 3 0 591 1433 293 0 830 2 1977 1 0 774 2 1395 0 9 242 0 34 42 1 2 396 4 0 272 78 959 205 27 2589 26 2410 2616 17 0 1752 1045 36 2240 463 46 483 33 104 1236 4 0 418 1769 1074 433 5852 1679 4654 310 82 1769 867 3913 679 104 1236 418 1769 1752 1045 2240 463 7931 1236 2589 1752 2240 1214 463 1\n",
            "609 511 0 9 1475 12 268 393 4033 4993 1715 3 207 28 3 107 266 11 8 658 42 4033 11 22 30 10 1013 11 8 1 1275 586 25 3432 8 428 11 22 35 30 759 180 11 8 62 142 5 3166 2211 893 5 127 35 48 3594 20 6283 1 0 236 8 555 44 927 4 3404 4 354 669 7932 11 22 35 3675 30 3038 6860 11 97 2925 24 11 22 30 5 779 45 6 0 216 1165 1 721 354 1 18 11 8641 5 779 3346 927 180 11 22 35 63 2212 6861 55 25 1295 1536 11 8 35 1523 25 5479 586 11 507 25 4800 207 50 89 440 1 11 8 1715 3 2425 4135 8326 750 4888 629 7099 1 3375 3 5 106 870 7 25 1252 116 3083 6460 207 11 3039 4 4079 155 28 318 8 401 1 1025 3285 8 460 1 1025 972 1631 368 2029 8 1025 11 22 5 1631 103 8 368 84 103 22 74 6862 2227 144 94 2527 228 4801 527 58 106 1193 2806 360 1861 928 521 7100 84 490 1434 368 2729 366 161 8 954 750 5361 156 3140 2443 126 4513 5731 1380 7331 33 10 2228 1039 20 2444 1252 1946 156 647 10 280 619 10 1638 11 22 1194 2283 6 126 313 686 126 8326 236 44 0 750 111 192 10 415 571 486 8 754 11 22 101 373 47 22 4994 1285 8 4381 119 1 62 10 169 344 41 183 390 178 587 268 393 4033 386 4800 207 49 60 1716 1 3433 558 61 893 5 127 11 2 142 0 1369 24 16 215 25 3432 3867 11 8 4 1724 2041 1122 1 1123 220 11 22 292 6 24 49 60 1716 6284 12 0 1806 54 1060 0 3595 7 25 718 24 1329 2 2125 23 42 23 101 12 3433 1 11 2706 60 32 3084 15 357 11 167 586 4 2844 2302 44 683 528 25 1295 620 1291 7 1 22 5 3676 47 8 1502 178 12 1807 638 1 11 24 7627 1524 2967 11 60 1123 1 675 220 11 1426 4 50 292 6 207 4307 7 268 279 12 25 959 1021 393 4033 4993 1687 748 1296 825 4993 207 4033 1715 3 207 4800 207 4033 4993 2\n",
            "34 147 458 456 1161 1 2870 4308 3113 963 2490 224 45 462 330 0 302 510 1 1166 3 0 34 14 239 6 0 9 0 9 2 133 4247 4 3677 734 834 0 34 484 454 1 0 611 302 0 523 3 0 34 239 477 276 180 1194 1725 2506 1543 1013 172 1659 405 388 422 1 376 1 3167 143 2203 6 378 3 663 1999 3 116 43 54 462 0 9 3 1595 143 2203 20 4080 4 0 116 4514 76 260 0 34 317 7628 4726 5593 497 4515 5362 1 6863 0 9 2 462 134 4382 1 7 4034 0 9 1557 0 462 330 1 3251 4 50 0 34 65 34 883 1169 1 290 189 14 1469 4187 316 0 34 0 9 1047 1347 316 0 34 7 975 4 7629 1 1476 5732 0 73 410 5853 2 5155 1 1206 4 6461 0 1048 3376 0 9 2 39 7 5 1862 98 15 0 259 397 6 5 5071 186 0 86 1 161 2042 0 64 93 1 1427 0 259 99 2 1735 6 1136 0 99 2 2023 6 152 3556 2617 5 148 2145 1374 12 173 425 1048 2 76 4 1808 0 1036 3915 3 0 4802 1 0 2490 1348 1 0 1680 173 1578 0 64 261 125 1 200 188 0 1680 1578 14 1790 6 27 512 6 1048 5 198 1170 84 110 4309 7332 5156 173 2 2030 2392 68 0 2254 565 0 1 0 4802 12 0 466 3725 3 458 1 0 120 1567 3 0 113 2445 1435 12 0 120 1428 3 456 1161 1 2870 54 1286 1270 14 76 4 2082 1693 173 425 0 264 451 2 76 4 2680 701 425 385 1736 2 251 17 26 4889 6 27 2750 10 255 1165 2 57 17 531 214 26 4583 264 16 0 173 964 2 71 1 5 2968 3286 27 512 2 1533 17 595 184 53 2968 884 6 10 58 675 37 40 253 3 27 512 2 526 4 5072 0 120 704 1 0 1427 125 53 1188 5 280 2 946 17 24 184 6 5 493 3 725 659 12 871 1818 66 526 116 14 4383 413 152 451 2 76 316 0 34 520 1228 179 171 179 1370 265 34 290 189 1 2174 14 150 0 9 2 562 6 950 4 2229 0 593 287 23 705 12 227 198 367 23 2446 23 893 209 945 12 0 213 127 1 4 1351 2183 1171 12 27 127 0 9 2 976 4 3141 66 116 0 9 2 976 4 32 7 3596 687 12 27 127 55 18 305 3141 66 33 1171 0 9 2 1206 4 4655 1914 90 303 12 499 3 363 86 45 500 940 408 45 398 646 20 385 324 20 324 7 216 20 175 450 349 88 1460 2 114 17 4803 535 295 645 7 132 101 4 100 279 82 2490 224 45 2490 7332 5156 173 1048 173 425 4308 3113 3113 125 512 173 1\n",
            "236 205 27 210 1082 181 85 6 185 286 471 26 210 406 743 6 182 3113 166 15 1096 259 36 1089 232 46 3225 28 3 452 319 6 182 617 3 0 21 104 420 40 1268 1 807 28 3 107 266 1228 41 107 7 0 3824 804 1671 203 386 0 9 8 5 2681 75 77 2393 103 965 7 6 408 499 3 363 3 101 127 2750 11 666 28 3 86 45 20 1237 20 635 6 1877 2707 17 24 56 11 2 7101 563 6 5 386 3 627 3 7630 12 3114 246 11 54 576 3115 9549 558 61 1 1132 2347 6 2311 1 25 5480 310 2 54 6 8642 11 557 4 2618 1096 12 25 210 406 743 25 105 176 2 916 1 2 1586 7 0 5995 5481 15 0 796 127 3 478 11 440 1396 286 471 6 1352 1 6677 2 1271 4 32 1514 1 5594 11 2 991 15 3142 4248 44 27 1525 4 26 83 1508 11 2 1061 15 26 83 1 991 15 7333 2491 0 1381 14 2348 408 44 84 1 40 4 227 3 7333 2491 1 40 3 2213 2491 6 5157 3 1413 17 0 56 3 1799 4 25 54 4656 4 0 2183 1 1819 310 236 116 9549 558 61 4081 202 153 2311 26 40 61 7631 721 46 409 5363 4 32 7631 721 83 409 6462 1153 6864 7631 893 721 135 409 1096 40 61 4081 202 153 3474 6678 1137 61 4081 202 153 1799 950 0 9 8 4 32 2551 15 7333 184 2491 3 227 43 2491 2213 2491 3 40 117 1 5157 3 1413 12 485 3 0 444 260 0 127 11 215 32 991 15 313 978 26 4 36 2230 209 752 6 38 1737 1169 3 871 1065 17 2184 444 315 1 24 8 4 32 1004 4 27 26 409 721 2184 15 478 25 898 30 669 4 40 40 1 6677 25 2146 4654 30 4 32 2126 49 215 4995 51 241 17 5 875 735 202 25 58 106 638 8 746 220 1271 236 1290 210 1082 181 85 286 471 406 743 210 406 743 408 499 3 363 499 3 363 408 499 1096 406 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Vocabulary and Dataset Files :**\n",
        "\n",
        "This code allows users to download the processed files from the Colab environment to their local machine. By importing files from google.colab, each files.download() call triggers a browser download for the specified file.\n",
        "\n",
        "The files downloaded are:\n",
        "\n",
        "* vocab.txt – top 10,000 words with IDs and frequencies.\n",
        "\n",
        "* train_ids.txt – training texts converted to word IDs with labels.\n",
        "\n",
        "* valid_ids.txt – validation texts in ID format.\n",
        "\n",
        "* test_ids.txt – test texts in ID format.\n",
        "\n",
        "This ensures that all prepared data and vocabulary can be saved locally for further use or inspection."
      ],
      "metadata": {
        "id": "sbVzNfwW4VMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"vocab.txt\")\n",
        "files.download(\"train_ids.txt\")\n",
        "files.download(\"valid_ids.txt\")\n",
        "files.download(\"test_ids.txt\")"
      ],
      "metadata": {
        "id": "XTIgm9YP2zwn",
        "outputId": "bcd19636-fb3b-4c58-846c-d7b87f54da69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c9bfd680-8a97-4639-bcdf-44a3cc07d1fb\", \"vocab.txt\", 166230)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef1ddd28-4127-4df0-b246-7f3a6db5dd64\", \"train_ids.txt\", 6997118)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c9cc44b0-2978-4910-9e84-9a340d638c20\", \"valid_ids.txt\", 871156)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4c93e765-e0ab-40c5-bdaa-3fe30c07939e\", \"test_ids.txt\", 884702)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorizing Texts into Binary and Frequency Bag-of-Words :**\n",
        "\n",
        "This code defines two functions for converting medical transcripts into numerical vectors, which are required for training machine learning classifiers. The first function, vectorize_BBoW, implements a Binary Bag-of-Words (BBoW) representation. It initializes a zero matrix of shape (number of texts × vocabulary size) and iterates through each text. For every unique word in the text, it checks if the word exists in the vocabulary (word2id). If it does, the corresponding column in the matrix is set to 1, indicating the presence of that word. This results in a binary vector where each dimension represents whether a vocabulary word appears in the text.\n",
        "\n",
        "The second function, vectorize_FBoW, implements a Frequency Bag-of-Words (FBoW) representation. It also initializes a zero matrix, but the entries are floats. For each text, it counts the occurrences of each word using Counter, then divides the count by the total number of words in the text to compute the relative frequency. If a word exists in the vocabulary, its corresponding column in the matrix is set to this normalized frequency. This results in a vector where each dimension represents how frequently a vocabulary word occurs in the text.\n",
        "\n",
        "Both functions use tqdm to display a progress bar during vectorization, which is helpful for large datasets. vectorize_BBoW produces sparse binary vectors suitable for algorithms like logistic regression, while vectorize_FBoW captures more nuanced word usage patterns, which can improve classifier performance in some cases."
      ],
      "metadata": {
        "id": "hhLApZ3243OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vectorization Functions\n",
        "def vectorize_BBoW(texts):\n",
        "    X = np.zeros((len(texts), len(vocab)), dtype=np.uint8)\n",
        "    for i, t in enumerate(tqdm(texts, desc=\"BBoW\")):\n",
        "        for word in set(t.split()):\n",
        "            idx = word2id.get(word)\n",
        "            if idx is not None:\n",
        "                X[i, idx] = 1\n",
        "    return X\n",
        "def vectorize_FBoW(texts):\n",
        "    X = np.zeros((len(texts), len(vocab)), dtype=np.float32)\n",
        "    for i, t in enumerate(tqdm(texts, desc=\"FBoW\")):\n",
        "        words = t.split()\n",
        "        total = len(words)\n",
        "        if total == 0:\n",
        "            continue\n",
        "        counts = Counter(words)\n",
        "        for word, c in counts.items():\n",
        "            idx = word2id.get(word)\n",
        "            if idx is not None:\n",
        "                X[i, idx] = c / total\n",
        "    return X"
      ],
      "metadata": {
        "id": "eSj7g7S6mSoy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing Binary Bag-of-Words Matrices and Encoding Labels :**\n",
        "\n",
        "This code prepares the Binary Bag-of-Words (BBoW) feature matrices for the training, validation, and test datasets, and encodes the class labels into numerical format for machine learning models.\n",
        "\n",
        "First, the vectorize_BBoW function is applied to each dataset: train_texts, valid_texts, and test_texts. This converts the raw medical transcripts into fixed-length binary vectors, where each vector dimension corresponds to a word in the top 10,000 vocabulary and a value of 1 indicates the presence of that word in the text. The resulting matrices—X_train_BBoW, X_valid_BBoW, and X_test_BBoW—are ready to be used as input for classifiers.\n",
        "\n",
        "Next, the class labels, which are originally strings or categorical values, are encoded into integers using LabelEncoder from scikit-learn. The fit_transform method is applied on train_labels to learn the mapping and convert them into numerical labels. The same mapping is then applied to the validation and test labels using transform, ensuring consistency across all datasets."
      ],
      "metadata": {
        "id": "Th1JWEwA5gMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Prepare BBoW matrices\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "X_train_BBoW = vectorize_BBoW(train_texts)\n",
        "X_valid_BBoW = vectorize_BBoW(valid_texts)\n",
        "X_test_BBoW  = vectorize_BBoW(test_texts)\n",
        "le = LabelEncoder()\n",
        "train_labels_enc = le.fit_transform(train_labels)\n",
        "valid_labels_enc = le.transform(valid_labels)\n",
        "test_labels_enc  = le.transform(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_gZFQHTHFCd",
        "outputId": "23d64871-9fcf-4730-a0cc-089fb9842308"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BBoW: 100%|██████████| 4000/4000 [00:00<00:00, 6728.36it/s]\n",
            "BBoW: 100%|██████████| 499/499 [00:00<00:00, 6912.47it/s]\n",
            "BBoW: 100%|██████████| 500/500 [00:00<00:00, 7149.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluating Classifiers with Binary Bag-of-Words :**\n",
        "\n",
        "This code trains and evaluates four machine learning classifiers—Logistic Regression, Decision Tree, Random Forest, and XGBoost—using the Binary Bag-of-Words (BBoW) representation of medical transcripts. The goal is to convert raw text into numerical vectors and assess each model’s ability to classify transcripts into Surgery, Medical Records, Internal Medicine, or Other.\n",
        "\n",
        "The classifiers are stored in a dictionary models with carefully chosen hyperparameters. Logistic Regression uses C=0.3 and class_weight=\"balanced\" to manage regularization and class imbalance. The Decision Tree is limited to a depth of 20 with at least 5 samples per leaf. Random Forest uses 200 trees with max_depth=20, min_samples_leaf=5, and max_features=\"sqrt\" for generalization. XGBoost is set with 200 boosting rounds, a max depth of 6, and eval_metric='mlogloss' to optimize multi-class loss.\n",
        "\n",
        "The code trains each model on X_train_BBoW and predicts labels for training, validation, and test sets. Performance is measured using the macro-averaged F1-score, which treats all classes equally, making it suitable for imbalanced datasets. The printed scores allow comparison across models and help detect overfitting or underfitting.\n",
        "\n",
        "**Classifier Performance and F1-Score Analysis :**\n",
        "\n",
        "The F1-scores reveal distinct patterns of performance for each classifier. Logistic Regression achieves a high training F1 of 0.9076 but lower validation and test scores of 0.6879 and 0.7268, indicating some overfitting. The regularization parameter C=0.3 helps control overfitting, with smaller values providing stronger regularization. The Decision Tree shows a slightly lower training F1 of 0.8627 but improved validation and test scores of 0.7319 and 0.7530, suggesting that limiting the tree’s depth to 20 and setting a minimum of 5 samples per leaf effectively controls complexity and reduces overfitting. In contrast, the Random Forest performs poorly, with a training F1 of 0.6979 and validation/test scores around 0.50. Despite being an ensemble method, the combination of restricted depth, minimum samples per leaf, and max_features=\"sqrt\" appears to have caused underfitting, limiting its ability to capture patterns in the data. XGBoost, on the other hand, achieves the best validation and test performance, with scores of 0.7697 and 0.7925, while maintaining a high training F1 of 0.9079. Its hyperparameters, including 200 estimators and a max depth of 6, provide a balance between model complexity and generalization, and the gradient boosting mechanism allows it to capture complex patterns more effectively than single decision trees or the Random Forest in this dataset.\n",
        "\n",
        "**Role of Hyperparameters :**\n",
        "\n",
        "* Max Depth (max_depth): Controls the complexity of tree-based models; deeper trees can overfit, shallow trees may underfit.\n",
        "\n",
        "* Min Samples Leaf (min_samples_leaf): Prevents splits that create very small leaf nodes, reducing overfitting.\n",
        "\n",
        "* Number of Estimators (n_estimators): More trees usually improve performance but increase computation.\n",
        "\n",
        "* Regularization (C in logistic regression): Controls overfitting by penalizing large coefficients.\n",
        "\n",
        "* Max Features (max_features): Limits the number of features considered at each split in Random Forests; can improve generalization but may reduce training performance."
      ],
      "metadata": {
        "id": "5ynOB8026661"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train & Evaluate (BBoW)\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000,C=0.3,class_weight=\"balanced\",solver=\"liblinear\"),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(max_depth=20,min_samples_leaf=5,random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, max_depth=20,max_features=\"sqrt\",min_samples_leaf=5,random_state=42,n_jobs=-1),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=6,eval_metric='mlogloss', random_state=42)}\n",
        "for vec_name, X_train, X_valid, X_test in [(\"BBoW\", X_train_BBoW, X_valid_BBoW, X_test_BBoW),]:\n",
        "    print(f\"\\n=== {vec_name} Results ===\")\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, train_labels_enc)\n",
        "        train_pred, valid_pred, test_pred = model.predict(X_train), model.predict(X_valid), model.predict(X_test)\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"Train F1: {f1_score(train_labels_enc, train_pred, average='macro'):.4f}\")\n",
        "        print(f\"Valid F1: {f1_score(valid_labels_enc, valid_pred, average='macro'):.4f}\")\n",
        "        print(f\"Test F1: {f1_score(test_labels_enc, test_pred, average='macro'):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_aCRIiqNpXt",
        "outputId": "fab9d077-841d-4c5f-c822-e7782b670aa2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BBoW Results ===\n",
            "\n",
            "LogisticRegression:\n",
            "Train F1: 0.9076\n",
            "Valid F1: 0.6879\n",
            "Test F1: 0.7268\n",
            "\n",
            "DecisionTree:\n",
            "Train F1: 0.8627\n",
            "Valid F1: 0.7319\n",
            "Test F1: 0.7530\n",
            "\n",
            "RandomForest:\n",
            "Train F1: 0.6979\n",
            "Valid F1: 0.4998\n",
            "Test F1: 0.5181\n",
            "\n",
            "XGBoost:\n",
            "Train F1: 0.9079\n",
            "Valid F1: 0.7697\n",
            "Test F1: 0.7925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing Frequency Bag-of-Words Matrices :**\n",
        "\n",
        "This code prepares the Frequency Bag-of-Words (FBoW) feature matrices for the training, validation, and test datasets. Each text is converted into a fixed-length vector using the vectorize_FBoW function, where each vector dimension corresponds to a word in the top 10,000 vocabulary. Unlike the binary representation, each entry in these vectors represents the relative frequency of the word in the text, capturing how often a word appears rather than just its presence."
      ],
      "metadata": {
        "id": "tu8P9pqc6roQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Prepare FBoW matrices\n",
        "X_train_FBoW = vectorize_FBoW(train_texts)\n",
        "X_valid_FBoW = vectorize_FBoW(valid_texts)\n",
        "X_test_FBoW  = vectorize_FBoW(test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJmqIUj-vZnj",
        "outputId": "2f1475e6-7e9e-434f-c662-05639eb558a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FBoW: 100%|██████████| 4000/4000 [00:00<00:00, 5230.05it/s]\n",
            "FBoW: 100%|██████████| 499/499 [00:00<00:00, 5453.63it/s]\n",
            "FBoW: 100%|██████████| 500/500 [00:00<00:00, 4928.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluating Classifiers with Frequency Bag-of-Words :**\n",
        "\n",
        "This code trains and evaluates four machine learning classifiers—Logistic Regression, Decision Tree, Random Forest, and XGBoost—using the Frequency Bag-of-Words (FBoW) representation of medical transcripts. Similar to the BBoW evaluation, a dictionary models stores each classifier with its hyperparameters, tuned for the FBoW vectors.\n",
        "\n",
        "The code iterates over the FBoW datasets (X_train_FBoW, X_valid_FBoW, X_test_FBoW). For each model, it is trained on the training set using .fit() and then predicts labels for training, validation, and test sets using .predict(). Performance is evaluated using the macro-averaged F1-score, which treats all classes equally, making it suitable for imbalanced data. The F1-scores are printed to compare how well each classifier generalizes when using word frequency information instead of binary presence.\n",
        "\n",
        "**Analysis of Results :**\n",
        "\n",
        "The F1-scores show significant differences in performance among the classifiers. Logistic Regression performs poorly, with Train F1 of 0.3253 and similar validation and test scores, indicating that it struggles with the frequency-based representation. Although the regularization parameter C=0.4 and L2 penalty are intended to prevent overfitting, the linear model is too simple to capture the complex relationships in FBoW data. Decision Tree performs much better, achieving a training F1 of 0.8364 and validation/test scores of 0.7028 and 0.7306. Hyperparameters such as max_depth=18 and min_samples_leaf=8 help control tree complexity and prevent overfitting, allowing the model to generalize effectively. Random Forest shows lower performance, with training F1 of 0.7361 and validation/test scores around 0.46–0.47. Despite being an ensemble method, the combination of limited tree depth, minimum samples per leaf, and max_features=\"sqrt\" seems to restrict the model, resulting in underfitting and reduced ability to capture patterns in the FBoW data. XGBoost achieves the best performance, with a training F1 of 0.9090 and validation/test scores of 0.7619 and 0.8075. Its hyperparameters, including n_estimators=200 and max_depth=6, balance model complexity and generalization, while the gradient boosting mechanism sequentially corrects errors, allowing it to capture nuanced patterns more effectively than the other models.\n",
        "\n",
        "**Role of Hyperparameters :**\n",
        "\n",
        "* C (Logistic Regression): Controls regularization; smaller values reduce overfitting, but here the model is too simple for FBoW features.\n",
        "\n",
        "* max_depth (Tree-based models): Limits tree complexity to prevent overfitting.\n",
        "\n",
        "* min_samples_leaf (Tree-based models): Ensures sufficient data in leaves for better generalization.\n",
        "\n",
        "* n_estimators (Random Forest/XGBoost): Number of trees; more trees improve performance but increase computation.\n",
        "\n",
        "* max_features (Random Forest): Controls feature sampling for splits, balancing diversity and bias."
      ],
      "metadata": {
        "id": "KgqU7eSr_MYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train & Evaluate (FBoW)\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=2000,C=0.4,class_weight=\"balanced\",solver=\"liblinear\",penalty=\"l2\"),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(max_depth=18,min_samples_leaf=8,random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, max_depth=20,max_features=\"sqrt\",min_samples_leaf=5,random_state=42,n_jobs=-1),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=6,eval_metric='mlogloss', random_state=42)}\n",
        "for vec_name, X_train, X_valid, X_test in [(\"FBoW\", X_train_FBoW, X_valid_FBoW, X_test_FBoW),]:\n",
        "    print(f\"\\n=== {vec_name} Results ===\")\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, train_labels_enc)\n",
        "        train_pred, valid_pred, test_pred = model.predict(X_train), model.predict(X_valid), model.predict(X_test)\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"Train F1: {f1_score(train_labels_enc, train_pred, average='macro'):.4f}\")\n",
        "        print(f\"Valid F1: {f1_score(valid_labels_enc, valid_pred, average='macro'):.4f}\")\n",
        "        print(f\"Test F1: {f1_score(test_labels_enc, test_pred, average='macro'):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmG2wMXH2_wG",
        "outputId": "4281e426-cc3f-49e6-d041-942e5d33273e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FBoW Results ===\n",
            "\n",
            "LogisticRegression:\n",
            "Train F1: 0.3253\n",
            "Valid F1: 0.3217\n",
            "Test F1: 0.3178\n",
            "\n",
            "DecisionTree:\n",
            "Train F1: 0.8364\n",
            "Valid F1: 0.7028\n",
            "Test F1: 0.7306\n",
            "\n",
            "RandomForest:\n",
            "Train F1: 0.7361\n",
            "Valid F1: 0.4639\n",
            "Test F1: 0.4735\n",
            "\n",
            "XGBoost:\n",
            "Train F1: 0.9090\n",
            "Valid F1: 0.7619\n",
            "Test F1: 0.8075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion: Comparison of BBoW and FBoW**\n",
        "\n",
        "Based on the F1-scores, the Binary Bag-of-Words (BBoW) and Frequency Bag-of-Words (FBoW) representations show different strengths across classifiers. For simpler linear models like Logistic Regression, BBoW clearly outperforms FBoW, achieving much higher validation and test scores (Valid F1: 0.6879 vs. 0.3217, Test F1: 0.7268 vs. 0.3178), suggesting that binary presence of words is more informative for linear classification than their frequency.\n",
        "\n",
        "For tree-based models, both representations perform reasonably well, but the differences are subtler. Decision Trees and XGBoost show slightly better or comparable results with FBoW, likely because these models can leverage word frequency information to capture nuanced patterns. For instance, XGBoost achieves the highest test F1 with FBoW (0.8075) compared to BBoW (0.7925), indicating that frequency information provides a marginal advantage for complex, non-linear models capable of handling richer feature interactions.\n",
        "\n",
        "**Overall**, BBoW tends to perform better for simpler models, while FBoW can provide slight improvements for more sophisticated tree-based models like XGBoost. This suggests that the choice of representation should consider both the type of classifier and the dataset characteristics: BBoW for linear models and FBoW for models capable of exploiting frequency information."
      ],
      "metadata": {
        "id": "M-oLzCl2BmWt"
      }
    }
  ]
}